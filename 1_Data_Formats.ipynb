{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data can exist in all shapes and sizes. We will start by looking at NOAA's [Tides & Currents](https://tidesandcurrents.noaa.gov/) dataset as presented in different data formats. Specifically, we will be looking at water levels as used to represent local tide information.\n",
    "\n",
    "<div style=\"text-align:center\"><img src='images/water_dash.png' width=500></div>\n",
    "\n",
    "## Objectives\n",
    "* Learn the differences between the following file types and when best to use them:\n",
    " * [JSON](https://www.json.org/json-en.html)\n",
    " * [CSV](https://datahub.io/docs/data-packages/csv/)\n",
    " * [Parquet](https://parquet.apache.org/documentation/latest/)\n",
    "* Learn the differences between the following libraries/framework and when best to use them:\n",
    " * [pandas](https://pandas.pydata.org/)\n",
    " * [cuDF](https://docs.rapids.ai/api/cudf/stable/)\n",
    " * [Dask](https://dask.org/)\n",
    " \n",
    "First, let's get these libraries loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.perf_counter()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let us pull our data directly from NOAA's [CO-OPS API](https://api.tidesandcurrents.noaa.gov/api/prod/). In particular, we will be looking at the water level of a body of water of your choosing.\n",
    "\n",
    "As a web API, we will need to craft a [Uniform Resource Locator](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL) (URL) in order to retrieve data from it. This API is flexible and uses several parameters:\n",
    "\n",
    "* `station`: A numerical ID of the station doing the surveying.\n",
    "* `range`: How many hours back to retrieve the data from the current time.\n",
    "* `units`: Can be `english` or `metric`.\n",
    "* `datum`: The [reference elevation](https://tidesandcurrents.noaa.gov/datum_options.html) to use.\n",
    "  * Different stations have a different set of datums available.\n",
    "* `format`: The type of file to return. Can be `json`, `xml`, or `csv`.\n",
    "\n",
    "**TODO**: Pick a station from NOAA's [Water Level Website](https://tidesandcurrents.noaa.gov/stations.html?type=Water+Levels) and update the `station` parameter below with its 7 digit ID number as a string. Many stations use the `MLLW` datum, but not all of them do. In order to find out which datums are available, use the dropdown at the bottom of the station's dashboard:\n",
    "\n",
    "<div style=\"text-align:center\"><img src='images/datum.png' width=500></div>\n",
    "\n",
    "**Hint**: If you receive a `400 Bad Request` error, click on the generated link to view the error message. Click the `...` for one possible solution.\n",
    "\n",
    "It should also be noted that this is a live data API and there is a chance that it may be experiencing downtime. In such case, we have saved a sample in [data/sample_json.txt](data/sample_json.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\"\n",
    "\n",
    "url_parameters = {\n",
    "    \"station\": \"FIXME\", # Add a station ID of your choosing. Please keep the quotes.\n",
    "    \"range\": 2,  # Please keep this in the single digits for now.\n",
    "    \"product\": \"water_level\",\n",
    "    \"units\": \"english\",\n",
    "    \"datum\": \"MLLW\",\n",
    "    \"time_zone\": \"gmt\",\n",
    "    \"application\": \"nvidia_dli\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "url = url_base + urllib.parse.urlencode(url_parameters)\n",
    "print(url)\n",
    "\n",
    "with urllib.request.urlopen(url) as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url_base = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\"\n",
    "\n",
    "url_parameters = {\n",
    "    \"station\": \"9063009\",\n",
    "    \"range\": 2,  # Please keep this in the single digits for now.\n",
    "    \"product\": \"water_level\",\n",
    "    \"units\": \"english\",\n",
    "    \"datum\": \"IGLD\",\n",
    "    \"time_zone\": \"gmt\",\n",
    "    \"application\": \"ports_screen\",\n",
    "    \"format\": \"json\"\n",
    "}\n",
    "\n",
    "url = url_base + urllib.parse.urlencode(url_parameters)\n",
    "print(url)\n",
    "\n",
    "with urllib.request.urlopen(url) as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, what does it mean? The result has been split into two main categories:\n",
    "* `metadata`: Overall information about the result.\n",
    " * `id`: Our chosen station ID.\n",
    " * `name`: The name of the station.\n",
    " * `lat`: The latitude of the station.\n",
    " * `lon`: The longitude of the station.\n",
    "* `data`: The water level readings within the past `2` hours.\n",
    " * `t`: The timestamp of the reading.\n",
    " * `v`: The value of the reading.\n",
    " * `s`: The standard deviation (Sigma) of the reading.\n",
    "   * If there a multiple values within a one second window, a standard deviation is given.\n",
    " * `f`: Four different quality flags.\n",
    "   * See the [documentation](https://coastwatch.pfeg.noaa.gov/erddap/info/nosCoopsWLR6/index.html) for more information.\n",
    " * `q`: Whether the data is (P)relimary or (V)erified.\n",
    " \n",
    "While this format is useful for building a web page, we can see that it is not compact. The field names for the data are repeated for each reading. (This is likely why NOAA uses one letter field names: to save space). In order to see how long it takes to load the data locally, let's grab more data and save it to `.txt` file.\n",
    "\n",
    "**TODO**: The CO-OPS API supports pulling data for the past year. Replace the FIXMEs below to pull a year's worth of data and save it. The [json](https://docs.python.org/3/library/json.html) documentation may be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_parameters[\"range\"] = 365 * 24\n",
    "url = url_base + urllib.parse.urlencode(url_parameters)\n",
    "\n",
    "with urllib.request.urlopen(url) as url:\n",
    "    data = json.FIXME(url.read().decode())\n",
    "\n",
    "with open(\"data/\" + url_parameters[\"station\"] + \".txt\", 'w') as outfile:\n",
    "    FIXME.dump(data[\"data\"], outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "url_parameters[\"range\"] = 365 * 24\n",
    "url = url_base + urllib.parse.urlencode(url_parameters)\n",
    "\n",
    "with urllib.request.urlopen(url) as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "\n",
    "with open(\"data/\" + url_parameters[\"station\"] + \".txt\", 'w') as outfile:\n",
    "    json.dump(data[\"data\"], outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the data file, we can use the [head](https://linux.die.net/man/1/head) command to avoid displaying the entire, potentially large, file. We can also use the [cut](https://linux.die.net/man/1/cut) command to cut each line down to 80 characters. Feel free to replace `sample_json` with your chosen station ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/sample_json.txt | cut -c -80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminal tools are handy, but if we want to do analysis on these files, it's best to load them into a [DataFrame](https://pandas.pydata.org/pandas-docs/stable/user_guide/dsintro.html#dataframe) which is like a spreadsheet in a programming object.\n",
    "\n",
    "[pandas](https://pandas.pydata.org/) and [cuDF](https://docs.rapids.ai/api/cudf/stable/) are similar libraries for reading and manipulating DataFrames, with one key difference: `pandas` loads data and performs computations on the CPU whereas `cuDF` does so on the GPU. Let's compare how quickly both of these library reads JSON.\n",
    "\n",
    "**TODO**: Replace `data/sample_json.txt` with the path to your downloaded station data. Run the below cell **twice**. The read time for GPU should drop dramatically as cuDF is initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_json.txt\"  # Replace with your file path\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = pd.read_json(file_path, orient=\"records\")\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = cudf.read_json(file_path)\n",
    "\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"json\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading is useful, but so is the ability to write. Let's convert our data into CSV, parquet, and ZIP: three other file formats that we will be exploring.\n",
    "\n",
    "First, [to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.csv\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu.to_csv(file_path, index=False)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu.to_csv(file_path, index=False)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"csv\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, [to_parquet](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.parquet\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu.to_parquet(file_path, index=False)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu.to_parquet(file_path, index=False)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"parquet\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to zip, using the `compression` parameter of [to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.csv.zip\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu.to_csv(file_path, compression=\"zip\", index=False)\n",
    "# cuDF does not yet support compression\n",
    "\"{:10s} pd: {:>8.5f}s cudf: ---\".format(\"csv\", t_pd.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next file format we will look at is the [CSV](https://datahub.io/docs/data-packages/csv) file type. CSV stands for Comma Separated Values. Most people are familiar with this format from spreadsheet editing tools like Microsoft [Excel](https://www.microsoft.com/en-us/microsoft-365/excel) and Google [Sheets](https://www.google.com/sheets/about/). CSVs are human readable and are stored in a text file. Let's take a look at the top few rows of our `sample_data.csv` using the [head](https://linux.die.net/man/1/head) command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/sample_data.csv | cut -c -80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSVs may or may not have a `header` row at the top like we do here. This row contains the column names.\n",
    "\n",
    "Let's compare the time to read a CSV file with pandas and cuDF two libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.csv\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = pd.read_csv(file_path)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = cudf.read_csv(file_path)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"csv\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the terminal `head` command, our DataFrames have a `head` method so we can see the first few rows. Let's verify that both the CPU and GPU version of our DataFrame look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the number of rows in the DataFrame using python's `len` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next file format we will look at is Apache's [parquet](https://parquet.apache.org/) format. Whereas CSV files are optimized for rows, `parquet` is optimized for columns. We can look at the raw data file, but it's not human readable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/sample_data.parquet | cut -c -80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parquet` files have a header and footer describing the schema of the file. The columns are broken down into chunks. A program that reads a `parquet` file would look at the metadata to figure out where the columns are in the file that is being called by the user. Read more about it [here](https://parquet.apache.org/documentation/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, let's see how long it takes `pandas` to read a `parquet` versus `cuDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.parquet\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = pd.read_parquet(file_path)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = cudf.io.parquet.read_parquet(file_path)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"parquet\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip\n",
    "\n",
    "Oftentimes, data is compressed to save space. [ZIP](https://experience.dropbox.com/resources/what-is-a-zip-file) works by encoding redundant data into fewer bytes. In many cases, data needs to be uncompressed before it is usable by other applications, but both `pandas` and `cuDF` can read this file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.csv.zip\"\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = pd.read_csv(file_path, compression=\"zip\")\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = cudf.read_csv(file_path, compression=\"zip\")\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\n",
    "    \"csv(zip)\", t_pd.interval, t_cudf.interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Reading\n",
    "\n",
    "[Dask](https://dask.org/) is an open-source library designed to natively scale Python code. Dask is a task-based library for parallel scheduling and execution. Although it is certainly possible to use the task-scheduling machinery directly to implement customized parallel workflows, most users only interact with Dask through a *Dask Collection API*.  The most popular \"collection\" APIs include:\n",
    "\n",
    "- [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html): Dask-based version of the [Pandas](https://pandas.pydata.org/) DataFrame/Series API.  Note that `dask_cudf` is just a wrapper around this collection module (`dask.dataframe`).\n",
    "- [Dask Array](https://docs.dask.org/en/latest/array.html): Dask-based version of the [NumPy](https://numpy.org/) array API\n",
    "- [Dask Bag](https://docs.dask.org/en/latest/bag.html): *Similar to* a Dask-based version of PyToolz or a Pythonic version of PySpark RDD\n",
    "\n",
    "For example, Dask DataFrame provides a convenient API for decomposing large pandas (or cuDF) DataFrame/Series objects into a collection of DataFrame *partitions*.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"images/dask-dataframe.svg\" width=\"350px\"></div>\n",
    "\n",
    "It is useful to understand (on a basic level) how Dask works. When an application or library uses a Dask collection API (like Dask DataFrame), they are typically using that API to construct a directed acyclic graph ([DAG](https://mathworld.wolfram.com/AcyclicDigraph.html)) of tasks.  Once a DAG is constructed, the **core** Dask API can be used (either directly or implicitly through the collection API) to schedule and execute the DAG on one or more threads/processes.\n",
    "\n",
    "In other words, Dask provides various APIs to:\n",
    "\n",
    "1. Construct a DAG of \"tasks\"\n",
    "2. Schedule/execute those DAGs\n",
    "3. (Optionally) Spin up dedicated worker and scheduler processes to enable distributed execution\n",
    "\n",
    "<div style=\"text-align:center\"><img src='images/dask_dag_cartoon.png' width=500></div>\n",
    "\n",
    "Let's take a look at all of this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/sample_data.csv\"\n",
    "\n",
    "with Timer() as t_dd_cpu:\n",
    "    ddf_cpu = dd.read_csv(file_path)\n",
    "with Timer() as t_dd_gpu:\n",
    "    ddf_gpu = dask_cudf.read_csv(file_path)\n",
    "\"{:10s} CPU: {:>8.5f}s GPU: {:>8.5f}s\".format(\n",
    "    \"csv(dask)\", t_dd_cpu.interval, t_dd_gpu.interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So fast!\n",
    "\n",
    "Well, not really. Let's take a look at the DataFrames to see what we mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of the DataFrames have numbers in them, but the structure is set up. This is the result of the [DAG](https://mathworld.wolfram.com/AcyclicDigraph.html). The structure is created to quickly produce a result, but there is no result yet. When we ran the `read_csv` functions, the data was not actually read yet.\n",
    "\n",
    "We can force a result with `compute`, causing the CSV files to be read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_cpu.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_gpu.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may notice that the time it takes to use `Dask`, regardless of CPU or GPU, is not as fast as regular `pandas` and `cuDF` in this case. That is because there some overhead setting up the DAG for parallelization. Because of this, reading data with Dask works best with a large amount of data split across multiple files.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Speaking of comparisons, what is better? `CSV` or `parquet`? `pandas` or `cuDF`? This depends on the data content and what we intend to use it for. In the `data` directory, `numbers.csv` is a large, randomly generated dataset with `7500` rows and `7500` columns. It has been duplicated and saved in `data/numbers.parquet`.\n",
    "\n",
    "Being a large file, we do not recommend opening it in a new tab as it may crash the notebook connection. Instead, we can sample the file programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/numbers.csv | cut -c -80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Play around with the `skip_rows` and `col_indexes` below to see how it impacts the execution time of both `pandas` and `cuDF`. Check out the documentation for [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) and [read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). For `parquet`, we are using the [pyarrow](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html) engine.\n",
    "\n",
    "**BONUS**: See if you can find a situation where pandas is faster than cuDF. What set of conditions made it possible? What happens when no rows are skipped or most rows are skipped? What happends when no columns are used or most columns are used?\n",
    "\n",
    "**Note**:\n",
    "* `skip_rows` works a little differently for [cuDF](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.io.csv.read_csv)\n",
    "* We've added a `row_num` column so the `parquet` rows can be filtered while read using [filters](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change these integers\n",
    "skip_rows = 1000  # Rows to skip, up to 7500\n",
    "col_indexes = range(0, 1000)  # Columns to include, up to 7500\n",
    "\n",
    "filter_rows = [(\"row_num\", \"<=\", skip_rows)]\n",
    "use_cols = [\"{0}\".format(col_idx) for col_idx in col_indexes]\n",
    "\n",
    "csv_path = \"data/numbers.csv\"\n",
    "parq_path = \"data/numbers.parquet\"\n",
    "\n",
    "# CSV\n",
    "with Timer() as t_pd:\n",
    "    pd.read_csv(csv_path, usecols=use_cols, skiprows=range(1, skip_rows + 1))\n",
    "with Timer() as t_cudf:\n",
    "    cudf.read_csv(csv_path, usecols=use_cols, header=0, skiprows=skip_rows)\n",
    "print(\n",
    "    \"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"csv\", t_pd.interval, t_cudf.interval)\n",
    ")\n",
    "\n",
    "# Parquet\n",
    "with Timer() as t_pd:\n",
    "    pd.read_parquet(parq_path, columns=use_cols, filters=filter_rows)\n",
    "with Timer() as t_cudf:\n",
    "    cudf.io.parquet.read_parquet(parq_path, columns=use_cols, filters=filter_rows)\n",
    "\n",
    "print(\n",
    "    \"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\n",
    "        \"parquet\", t_pd.interval, t_cudf.interval\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thought before moving on, how much space did each of these file types take? We can use the `ls` terminal command to list all the files in the `data` directory. The `s` flag lists the size and the `h` flag makes that size \"human readable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data -sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the sizes match what you would expect? How well do they reflect the speed it takes to read these files?\n",
    "Have answers to all of these questions? After this set of notebooks, return to the task launcher to take a short quiz. But for now, proceed to the [next notebook](2_Dask_and_MapReduce.ipynb) to learn more about Dask and MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
