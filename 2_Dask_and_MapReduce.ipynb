{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we learned how to read data with Dask, but we found that it was not faster than pandas or cuDF for a single file. In this lab, we will learn how to use Dask to speed up computation under the correct conditions.\n",
    "\n",
    "## Objectives\n",
    "* Learn when to use Dask\n",
    "* Learn the basics of MapReduce\n",
    " \n",
    "First, let's get these libraries loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import dask.dataframe as dd\n",
    "import dask_cudf\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.start = time.perf_counter()\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.end = time.perf_counter()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither pandas or cuDF can read in multiple CSV files directly with [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html). In order to read multiple files into a DataFrame, we would need to loop through each file and append them together.\n",
    "\n",
    "To see this, let's pull a couple more files from the [Water Level Website](https://tidesandcurrents.noaa.gov/stations.html?type=Water+Levels). This time, we will request a CSV and save it with the [urllib.request](https://docs.python.org/3/library/urllib.request.html).\n",
    "\n",
    "**TODO**: Pull 2 more stations of data with the [CO-OPS API](https://api.tidesandcurrents.noaa.gov/api/prod/). In the event the website is down, we have saved sample data into [data/sample_data2.csv](data/sample_data2.csv) and [data/sample_data3.csv](data/sample_data3.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = \"https://api.tidesandcurrents.noaa.gov/api/prod/datagetter?\"\n",
    "\n",
    "station_id = \"CHANGE_ME\"\n",
    "\n",
    "url_parameters = {\n",
    "    \"station\": station_id,\n",
    "    \"range\": 365 * 24,  # 1 year of data.\n",
    "    \"product\": \"water_level\",\n",
    "    \"units\": \"english\",\n",
    "    \"datum\": \"MLLW\",\n",
    "    \"time_zone\": \"gmt\",\n",
    "    \"application\": \"ports_screen\",\n",
    "    \"format\": \"csv\"\n",
    "}\n",
    "\n",
    "url = url_base + urllib.parse.urlencode(url_parameters)\n",
    "urllib.request.urlretrieve(url, \"data/\" + station_id + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have a few `.csv` files in the `data` folder. When referencing these files, we could type out the paths of each of these files individually, but instead, we will use the [glob](https://docs.python.org/3/library/glob.html) library to programmatically do this for us. We can use `*` as a wild card to filter files that match our pattern specified like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"data/*.csv\")\n",
    "file_paths = [file for file in file_paths if file != \"data/numbers.csv\"]\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each path starts with `data`, ends with `.csv`, and the `*` indicates to pick up anything in between. Let's set up a for loop to see how long it takes to read all of these files. Run the block **twice** to see how much faster cuDF is after it has been initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = [0, 1, 2]  # Column names are different when pulling csv directly\n",
    "\n",
    "\n",
    "def read_all(library, file_paths):\n",
    "    df_list = []\n",
    "    for file in file_paths:\n",
    "        df = library.read_csv(\n",
    "            file, index_col=None, header=None, usecols=usecols, skiprows=1\n",
    "        )\n",
    "        df_list.append(df)\n",
    "    return library.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "with Timer() as t_pd:\n",
    "    df_cpu = read_all(pd, file_paths)\n",
    "with Timer() as t_cudf:\n",
    "    df_gpu = read_all(cudf, file_paths)\n",
    "\"{:10s} pd: {:>8.5f}s cudf: {:>8.5f}s\".format(\"csv\", t_pd.interval, t_cudf.interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Dask is made to be parallel, we do not need a for loop. It can read multiple files natively.\n",
    "\n",
    "**TODO**: The below code shows how to read data in parallel. As we might remember from the previous notebook, this only sets up the process to read the files. Uncomment the two `FIXME` lines and replace the `FIXME` with the correct method to force Dask to *compute* a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer() as t_dd_cpu:\n",
    "    ddf_cpu = dd.read_csv(file_paths, usecols=usecols, header=0, skipinitialspace=True)\n",
    "    #ddf_cpu.FIXME()\n",
    "with Timer() as t_dd_gpu:\n",
    "    ddf_gpu = dask_cudf.read_csv(file_paths, usecols=usecols, header=0)\n",
    "    #ddf_gpu.FIXME()\n",
    "    # skipinitialspace not supported in dask_cudf\n",
    "    ddf_gpu.columns = [\"Date Time\", \"Water Level\", \"Sigma\"]\n",
    "\"{:10s} CPU: {:>8.5f}s GPU: {:>8.5f}s\".format(\n",
    "    \"csv(dask)\", t_dd_cpu.interval, t_dd_gpu.interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "with Timer() as t_dd_cpu:\n",
    "    ddf_cpu = dd.read_csv(file_paths, usecols=usecols, header=0, skipinitialspace=True)\n",
    "    ddf_cpu.compute()\n",
    "with Timer() as t_dd_gpu:\n",
    "    ddf_gpu = dask_cudf.read_csv(file_paths, usecols=usecols, header=0)\n",
    "    ddf_gpu.compute()\n",
    "    # skipinitialspace not supported in dask_cudf\n",
    "    ddf_gpu.columns = [\"Date Time\", \"Water Level\", \"Sigma\"]\n",
    "\"{:10s} CPU: {:>8.5f}s GPU: {:>8.5f}s\".format(\n",
    "    \"csv(dask)\", t_dd_cpu.interval, t_dd_gpu.interval\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample our data to confirm it had been read correctly. This time, we will only be working with the first three columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_cpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf_gpu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can Dask do this faster than regular pandas or cuDF? Under the hood, Dask is building a system of operations called a DAG. We can view this DAG with the [visualize](https://docs.dask.org/en/latest/graphviz.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_cpu.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_gpu.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see a number of circles with `read-csv` on them. Each circle is a [partition](https://docs.dask.org/en/latest/dataframe-design.html#dataframe-design-partitions) Dask has automatically assigned to read our data. In other words, we are dividing the work among multiple workers. In this case, it is likely that there is one circle for each file.\n",
    "\n",
    "For our GPU graph, we have an extra line of code to rename the columns:\n",
    "\n",
    "`ddf_gpu.columns = [\"Date Time\", \"Water Level\", \"Sigma\"]`\n",
    "\n",
    "This will show up as an extra node in the graph called `Rename`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce\n",
    "\n",
    "This is useful not only for reading files, but also for computation. We can assign each of our workers a function to carry out. There are two major types of functions we can assign our workers:\n",
    "\n",
    "* `Map` is a type of function that can be carried out by each worker individually.\n",
    "* `Reduce` is a type of function that requires all workers to share information.\n",
    "\n",
    "Let's work through an example to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_water_level = ddf_gpu[\"Water Level\"] + 10\n",
    "map_water_level.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're adding `10` to the water level. Since each worker does not need to see any of the other worker's data, this would be a `Map` operation.\n",
    "\n",
    "Alternatively, we can use the [map](https://docs.rapids.ai/api/cudf/stable/api.html#cudf.core.series.Series.map) method to apply our own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ten(x):\n",
    "    return x + 10\n",
    "\n",
    "map_water_level = ddf_gpu[\"Water Level\"].map(add_ten)\n",
    "map_water_level.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuDF has a few built in map functions. For example, if we have an address column and we want to break down its components (street, state/province, country) into multiple columns, we might want to use the [apply_rows](https://docs.rapids.ai/api/cudf/stable/api.html?#cudf.core.dataframe.DataFrame.apply_rows) or [apply_chunks](https://docs.rapids.ai/api/cudf/stable/api.html?#cudf.core.dataframe.DataFrame.apply_chunks) methods. Read more about them in [this article](https://medium.com/rapids-ai/user-defined-functions-in-rapids-cudf-2d7c3fc2728d).\n",
    "\n",
    "Unlike `Map` functions, `Reduce` functions aggregate data across multiple workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_water_level = ddf_gpu[\"Water Level\"].sum()\n",
    "reduce_water_level.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see all three workers reduce down to one `series-sum-agg` worker. In order to calculate a sum across all the data, the workers need to communicate to share their data.\n",
    "\n",
    "Because of this, `Map` functions can run asynchronously, whereas `Reduce` functions force synchronization. If it can be avoided, try to eliminate unnecessary `Reduce` functions to prevent idle workers. Aggregate functions like `sum` are unavoidable and must be done with a `Reduce` step.\n",
    "\n",
    "We can combine these two functions together to create `MapReduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce_water_level = (ddf_gpu[\"Water Level\"] + 10).sum()\n",
    "mapreduce_water_level.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order matters. If we reverse the operations, we will end up with a different graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducemap_water_level = ddf_gpu[\"Water Level\"].sum() + 10\n",
    "reducemap_water_level.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Think of your own MapReduce pipeline using `ddf_gpu`. Try to have 2 `Map` functions and 2 `Reduce` functions. We have created an outline below, but feel free to make a structure of your own.\n",
    "\n",
    "**Hints**:\n",
    " * The available columns are `Date Time`, `Water Level`, and `Sigma`\n",
    " * Some pandas aggregate functions are [sum](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html), [mean](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html), [min](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.min.html), and [max](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.max.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map1_ddf_gpu = ddf_gpu[\"Water Level\"] FIXME\n",
    "map2_ddf_gpu = ddf_gpu[\"Sigma\"] FIXME\n",
    "reduce1_ddf_gpu = map1_ddf_gpu FIXME\n",
    "reduce2_ddf_gpu = map2_ddf_gpu FIXME\n",
    "final = reduce1_ddf_gpu + reduce2_ddf_gpu\n",
    "final.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "map1_ddf_gpu = ddf_gpu[\"Water Level\"] ** 2\n",
    "map2_ddf_gpu = ddf_gpu[\"Sigma\"] / 10\n",
    "reduce1_ddf_gpu = map1_ddf_gpu.sum()\n",
    "reduce2_ddf_gpu = map2_ddf_gpu.mean()\n",
    "final = reduce1_ddf_gpu + reduce2_ddf_gpu\n",
    "final.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did your final graph end up looking like? It's interesting that with only six lines of code, we can already build a visually large graph. Want to further test your MapReduce knowledge? Please return to the task launcher to take a short quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
